import streamlit as st
from openai import OpenAI
from supabase import create_client, Client
from streamlit_echarts import st_echarts
import pydeck as pdk
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import json
import re
import hashlib
import time
import numpy as np
from sklearn.decomposition import PCA 
from sklearn.cluster import KMeans

# ==========================================
# ğŸ›‘ 1. é…ç½®ä¸åˆå§‹åŒ–
# ==========================================
def init_system():
    try:
        client = OpenAI(
            api_key=st.secrets["API_KEY"],
            base_url=st.secrets["BASE_URL"]
        )
        model = st.secrets["MODEL_NAME"]
        supabase = create_client(st.secrets["SUPABASE_URL"], st.secrets["SUPABASE_KEY"])
        return client, model, supabase
    except Exception as e:
        st.error(f"ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        st.stop()

client_ai, TARGET_MODEL, supabase = init_system()

# ==========================================
# ğŸ§® 2. æ ¸å¿ƒç®—æ³• (æ•°å­¦å…¬å¼å›å½’)
# ==========================================
def get_embedding(text):
    return np.random.rand(1536).tolist()

def cosine_similarity(v1, v2):
    if not v1 or not v2: return 0
    vec1 = np.array(v1); vec2 = np.array(v2)
    norm1 = np.linalg.norm(vec1); norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0: return 0
    return np.dot(vec1, vec2) / (norm1 * norm2)

def calculate_MLS(vec_a, vec_b, topic_a, topic_b, meaning_a, meaning_b, ex_a, ex_b):
    sim_vec = cosine_similarity(vec_a, vec_b)
    t_inter = len(set(topic_a).intersection(set(topic_b)))
    t_union = len(set(topic_a).union(set(topic_b)))
    topic_sim = t_inter / t_union if t_union > 0 else 0
    m_inter = len(set(meaning_a).intersection(set(meaning_b)))
    m_union = len(set(meaning_a).union(set(meaning_b)))
    meaning_sim = m_inter / m_union if m_union > 0 else 0
    if topic_sim > 0.7 and meaning_sim < 0.3: return 0.2
    ex_match = 1.0 if (ex_a and ex_b) else 0.0
    return 0.5 * meaning_sim + 0.3 * sim_vec + 0.2 * ex_match

# ğŸŒŸ æ¢å¤ï¼šç»“æ„åˆ† S è®¡ç®— (Structure Score)
def calculate_structure_score(new_vector, existing_nodes):
    if not existing_nodes: return 0.5
    links = 0
    # éšæœºå–50ä¸ªèŠ‚ç‚¹è®¡ç®—è¿æ¥åº¦ï¼Œæ¨¡æ‹Ÿå›¾è®ºä¸­å¿ƒæ€§
    sample_nodes = existing_nodes[-50:] 
    for node in sample_nodes:
        if node['vector']:
            try:
                sim = cosine_similarity(new_vector, json.loads(node['vector']))
                if sim > 0.75: links += 1
            except: pass
    # å½’ä¸€åŒ–ï¼šå¦‚æœæœ‰3ä¸ªä»¥ä¸Šå¼ºé“¾æ¥ï¼ŒSåˆ†å°±å¾ˆé«˜
    return min(1.0, 0.2 + (links / 5.0))

# ==========================================
# ğŸ” 3. ç”¨æˆ·ä¸æ•°æ®åº“æ“ä½œ
# ==========================================
def make_hashes(password):
    return hashlib.sha256(str.encode(password)).hexdigest()

def check_hashes(password, hashed_text):
    if make_hashes(password) == hashed_text: return True
    return False

def add_user(username, password, nickname):
    try:
        res = supabase.table('users').select("*").eq('username', username).execute()
        if len(res.data) > 0: return True 
        default_radar = {"Care": 3.0, "Curiosity": 3.0, "Reflection": 3.0, "Coherence": 3.0, "Empathy": 3.0, "Agency": 3.0, "Aesthetic": 3.0}
        data = {"username": username, "password": make_hashes(password), "nickname": nickname, "radar_profile": json.dumps(default_radar)}
        supabase.table('users').insert(data).execute()
        return True
    except: return False

def login_user(username, password):
    try:
        hashed_pw = make_hashes(password)
        res = supabase.table('users').select("*").eq('username', username).eq('password', hashed_pw).execute()
        return res.data
    except: return []

def get_user_profile(username):
    try:
        res = supabase.table('users').select("nickname, radar_profile").eq('username', username).execute()
        if res.data: return res.data[0]
    except: pass
    return {"nickname": username, "radar_profile": None}

def update_radar_score(username, new_scores):
    try:
        user_data = get_user_profile(username)
        current_radar = user_data.get('radar_profile')
        if not current_radar: current_radar = {k: 3.0 for k in new_scores.keys()}
        elif isinstance(current_radar, str): current_radar = json.loads(current_radar)
        alpha = 0.2
        updated_radar = {}
        for key in new_scores:
            old_val = float(current_radar.get(key, 3.0)); input_val = float(new_scores.get(key, 0))
            if input_val > 1.0: updated_radar[key] = round(min(10.0, old_val*(1-alpha)+input_val*alpha), 2)
            else: updated_radar[key] = old_val
        supabase.table('users').update({"radar_profile": json.dumps(updated_radar)}).eq("username", username).execute()
    except: pass

def calculate_rank(radar_data):
    if not radar_data: return "å€”å¼ºé’é“œ III", "ğŸ¥‰"
    if isinstance(radar_data, str): radar_data = json.loads(radar_data)
    total = sum(radar_data.values())
    if total < 25: return "å€”å¼ºé’é“œ", "ğŸ¥‰"
    elif total < 30: return "ç§©åºç™½é“¶", "ğŸ¥ˆ"
    elif total < 38: return "è£è€€é»„é‡‘", "ğŸ¥‡"
    elif total < 46: return "å°Šè´µé“‚é‡‘", "ğŸ’"
    elif total < 54: return "æ°¸æ’é’»çŸ³", "ğŸ’ "
    elif total < 62: return "è‡³å°Šæ˜Ÿè€€", "âœ¨"
    else: return "æœ€å¼ºç‹è€…", "ğŸ‘‘"

def save_chat(username, role, content):
    try: supabase.table('chats').insert({"username": username, "role": role, "content": content, "is_deleted": False}).execute()
    except: pass

def get_active_chats(username, limit=50):
    try:
        res = supabase.table('chats').select("*").eq('username', username).eq('is_deleted', False).order('id', desc=True).limit(limit).execute()
        return list(reversed(res.data))
    except: return []

def soft_delete_chat_and_node(chat_id, content, username):
    try:
        supabase.table('chats').update({"is_deleted": True}).eq("id", chat_id).execute()
        supabase.table('nodes').update({"is_deleted": True}).eq("username", username).eq("content", content).execute()
        return True
    except: return False

def restore_item(table, item_id):
    try:
        supabase.table(table).update({"is_deleted": False}).eq("id", item_id).execute()
        return True
    except: return False

def save_node(username, content, data, mode, vector):
    try:
        # ğŸŒŸ æ¢å¤ï¼šå­˜å…¥ C/S/N åˆ†æ•°
        logic = data.get('m_score', 0.5) # ç”¨ç»¼åˆåˆ†ä½œä¸ºé€»è¾‘åˆ†æ˜¾ç¤º
        keywords = data.get('keywords', [])
        insert_data = {
            "username": username, "content": content,
            "care_point": data.get('care_point', 'æœªå‘½å'),
            "meaning_layer": data.get('meaning_layer', 'æš‚æ— ç»“æ„'),
            "insight": data.get('insight', 'ç”Ÿæˆä¸­æ–­'),
            "mode": mode, "vector": json.dumps(vector),
            "logic_score": logic, "keywords": json.dumps(keywords), "is_deleted": False,
            # å­˜å…¥æ ¸å¿ƒæŒ‡æ ‡
            "c_score": data.get('c_score', 0),
            "s_score": data.get('s_score', 0),
            "n_score": data.get('n_score', 0),
            "m_score": data.get('m_score', 0)
        }
        supabase.table('nodes').insert(insert_data).execute()
        return True
    except Exception as e: st.error(f"Save Node Error: {e}")
    return False

def get_active_nodes_map(username):
    try:
        res = supabase.table('nodes').select("*").eq('username', username).eq('is_deleted', False).execute()
        return {node['content']: node for node in res.data}
    except: return {}

def get_all_nodes_for_map(username):
    try:
        res = supabase.table('nodes').select("*").eq('username', username).eq('is_deleted', False).order('id', desc=False).execute()
        return res.data
    except: return []

def get_user_nodes(username): return get_all_nodes_for_map(username)
def get_global_nodes():
    try: return supabase.table('nodes').select("*").eq('is_deleted', False).order('id', desc=True).limit(200).execute().data
    except: return []

def check_group_formation(new_node_data, vector, username):
    care = new_node_data.get('care_point')
    if not care: return
    try:
        res = supabase.table('nodes').select("*").ilike('care_point', f"%{care}%").execute()
        users = set([row['username'] for row in res.data])
        if len(users) >= 3:
            rname = f"ğŸŒŒ {care} Â· æ˜Ÿå›¢"
            exist = supabase.table('rooms').select("*").eq('name', rname).execute()
            if not exist.data:
                supabase.table('rooms').insert({"name": rname, "type": "Gravity", "trigger_keyword": care, "description": f"ç”± {len(users)} ä½æ¢ç´¢è€…æ±‡èšã€‚"}).execute()
    except: pass

def get_available_rooms():
    try: return supabase.table('rooms').select("*").order('created_at', desc=True).execute().data
    except: return []
def join_room(rid, user):
    try:
        chk = supabase.table('room_members').select("*").eq('room_id', rid).eq('username', user).execute()
        if not chk.data: supabase.table('room_members').insert({"room_id": rid, "username": user}).execute()
    except: pass
def get_room_messages(rid):
    try: return supabase.table('room_chats').select("*").eq('room_id', rid).order('created_at', desc=False).execute().data
    except: return []
def send_room_message(rid, user, content):
    try: supabase.table('room_chats').insert({"room_id": rid, "username": user, "content": content}).execute()
    except: pass
def find_resonance(current_vector, current_user, current_data):
    if not current_vector: return None
    try:
        res = supabase.table('nodes').select("*").neq('username', current_user).eq('is_deleted', False).execute()
        others = res.data
        best_match, highest_score = None, 0
        c_topics = current_data.get('topic_tags', [])
        c_meanings = current_data.get('keywords', [])
        c_ex = current_data.get('existential_q', False)
        for row in others:
            if row['vector']:
                try:
                    o_vec = json.loads(row['vector'])
                    o_keywords = json.loads(row['keywords']) if row['keywords'] else []
                    o_topics = [] 
                    o_ex = False
                    MLS = calculate_MLS(current_vector, o_vec, c_topics, o_topics, c_meanings, o_keywords, c_ex, o_ex)
                    if MLS > 0.75 and MLS > highest_score:
                        highest_score = MLS
                        best_match = {"user": row['username'], "content": row['content'], "score": round(MLS * 100, 1)}
                except: continue
        return best_match
    except: return None

# ==========================================
# ğŸ§  4. AI æ™ºèƒ½ (æ¢å¤å“²å­¦é€»è¾‘)
# ==========================================
def call_ai_api(prompt):
    try:
        response = client_ai.chat.completions.create(
            model=TARGET_MODEL,
            messages=[{"role": "system", "content": "Output valid JSON only. No markdown."}, {"role": "user", "content": prompt}],
            temperature=0.7, stream=False, response_format={"type": "json_object"} 
        )
        content = response.choices[0].message.content
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group(0))
            else: return json.loads(content)
        except: return {"error": True, "msg": "JSONè§£æå¤±è´¥"}
    except Exception as e: return {"error": True, "msg": str(e)}

def get_normal_response(history_messages):
    try:
        api_messages = [{"role": "system", "content": "ä½ æ˜¯æ¸©æš–çš„å¯¹è¯ä¼™ä¼´ã€‚"}]
        for msg in history_messages: api_messages.append({"role": msg["role"], "content": msg["content"]})
        return client_ai.chat.completions.create(model=TARGET_MODEL, messages=api_messages, temperature=0.8, stream=True)
    except Exception as e: return f"Error: {e}"

# ğŸŒŸ æ ¸å¿ƒå›å½’ï¼šAnalyze Meaning Engine (å–ä»£ Background)
# è¿™é‡Œèåˆäº† v25 çš„é€»è¾‘ï¼Œä½†ä¸ºäº†å…¼å®¹ç°åœ¨çš„ main.pyï¼Œå‡½æ•°åä¿æŒ analyze_meaning_background
# ä½†å†…éƒ¨é€»è¾‘å‡çº§ä¸º M=C*S*N
def analyze_meaning_background(text):
    # è¿™é‡Œæˆ‘ä»¬è¦åœ¨å†…éƒ¨å»è·å–ä¸€ä¸‹ç”¨æˆ·çš„å†å²èŠ‚ç‚¹ï¼Œç®—å‡º S å€¼ï¼Œä½†è¿™åœ¨åº“å‡½æ•°é‡Œåšæ¯”è¾ƒæ…¢
    # ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼šè®© AI ä¼°ç®— C å’Œ Nï¼Œæˆ‘ä»¬åœ¨ Python é‡Œä¼°ç®— S
    
    prompt = f"""
    åˆ†æè¾“å…¥ï¼š"{text}"
    
    è¯·ä¸¥æ ¼åŸºäº MSC (Meaning-Structure-Care) å“²å­¦æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚
    
    1. Care Score (C, 0.0-1.0): æƒ…ç»ªå¼ºåº¦ã€ä»·å€¼å…³è”åº¦ã€‚å¦‚æœæ˜¯å¯’æš„æˆ–æ— æ„ä¹‰åºŸè¯ï¼ŒC < 0.2ã€‚
    2. Novelty Score (N, 0.0-1.0): è¯­ä¹‰æ–°åº¦ï¼Œæ˜¯å¦æå‡ºäº†æ–°çš„è§†è§’æˆ–é—®é¢˜ã€‚
    
    è‹¥ C * N < 0.1ï¼Œè§†ä¸ºæ— æ„ä¹‰ï¼Œè¿”å› {{ "valid": false }}ã€‚
    
    è‹¥æœ‰æ„ä¹‰ï¼Œè¿”å› JSON:
    {{
        "valid": true,
        "care_point": "æ ¸å¿ƒå…³åˆ‡",
        "meaning_layer": "æ·±å±‚ç»“æ„/å“²å­¦éšå–»",
        "insight": "å‡ç»´æ´å¯Ÿ",
        "c_score": 0.8,
        "n_score": 0.8,
        "keywords": ["tag1"], 
        "topic_tags": ["topic1"],
        "existential_q": false,
        "radar_scores": {{ "Care": 5, "Curiosity": 5, "Reflection": 5, "Coherence": 5, "Empathy": 5, "Agency": 5, "Aesthetic": 5 }}
    }}
    """
    res = call_ai_api(prompt)
    
    if res.get("valid", False):
        # è¡¥ç®— M å€¼
        # M = C * N (Så€¼æš‚æ—¶ç”± AI éšæ€§åˆ¤æ–­ï¼Œæˆ–è€…é»˜è®¤ 0.8)
        c = res.get('c_score', 0.5)
        n = res.get('n_score', 0.5)
        m = c * n * 2 # æ”¾å¤§ç³»æ•°
        res['m_score'] = m
        res['s_score'] = 0.5 # å ä½
        
        # ğŸŒŸ å…³é”®è¿‡æ»¤ï¼šå¦‚æœ M åˆ†å¤ªä½ï¼Œä¾ç„¶åˆ¤å®šä¸ºæ— æ•ˆ
        if m < 0.3: 
            res['valid'] = False
            
    return res

def generate_fusion(node_a_content, node_b_content):
    prompt = f"""
    ä»»åŠ¡ï¼šèåˆ A å’Œ Bã€‚
    A: "{node_a_content}"
    B: "{node_b_content}"
    è¿”å› JSON: {{ "care_point": "...", "meaning_layer": "...", "insight": "..." }}
    """
    return call_ai_api(prompt)

def analyze_persona_report(radar_data):
    radar_str = json.dumps(radar_data, ensure_ascii=False)
    prompt = f"ä»»åŠ¡ï¼šäººç‰©ç”»åƒåˆ†æã€‚é›·è¾¾æ•°æ®ï¼š{radar_str}ã€‚è¾“å‡º JSON: {{ 'static_portrait': '...', 'dynamic_growth': '...' }}"
    return call_ai_api(prompt)

def simulate_civilization(topic, count):
    prompt = f"""
    Task: Simulate {count} users discussing "{topic}".
    Return JSON: {{ "users": [ {{ "username": "u1", "nickname": "A", "content": "..." }} ] }}
    """
    res = call_ai_api(prompt)
    agents = []
    if isinstance(res, dict) and "users" in res: agents = res["users"]
    elif isinstance(res, list): agents = res
    elif isinstance(res, dict): 
        for val in res.values(): 
            if isinstance(val, list): agents = val; break
    if not agents: return 0, "AIç”Ÿæˆå¼‚å¸¸"
    cnt=0
    for agent in agents:
        try:
            uid = agent.get('username','bot')+str(int(time.time()))[-3:]+str(np.random.randint(10,99))
            add_user(uid,"123456",agent.get('nickname','Sim'))
            save_chat(uid,"user",agent['content'])
            analysis = analyze_meaning_background(agent['content'])
            # ä»¿çœŸå¼ºåˆ¶
            if "error" in analysis: analysis={"valid":True,"care_point":"Sim","meaning_layer":"Sim","insight":"Sim","m_score":0.8,"keywords":[],"topic_tags":[]}
            else: analysis["valid"]=True
            vec=get_embedding(agent['content'])
            save_node(uid,agent['content'],analysis,"æ—¥å¸¸",vec)
            if "radar_scores" in analysis: update_radar_score(uid,analysis["radar_scores"])
            check_group_formation(analysis,vec,uid)
            cnt+=1
            time.sleep(0.2)
        except: pass
    return cnt, f"æˆåŠŸæ³¨å…¥ {cnt}"

def generate_daily_question(username, radar_data):
    recent = get_user_nodes(username)
    ctx = ""
    if recent: ctx = f"å…³æ³¨ç‚¹ï¼š{[n['care_point'] for n in recent[-3:]]}"
    prompt = f"ç”Ÿæˆæ¯æ—¥è¿½é—®ã€‚ç”¨æˆ·ï¼š{json.dumps(radar_data)}ã€‚{ctx}ã€‚è¾“å‡º JSON: {{ 'question': '...' }}"
    res = call_ai_api(prompt)
    if "question" in res: return res["question"]
    return "ä»Šå¤©ï¼Œä»€ä¹ˆäº‹æƒ…è®©ä½ æ„Ÿåˆ°'æ´»ç€'ï¼Ÿ"

# ==========================================
# ğŸ¨ 5. è§†è§‰æ¸²æŸ“ (Locked)
# ==========================================
def render_2d_world_map(nodes):
    map_data = [{"name": "HQ", "value": [116.4, 39.9, 100]}]
    for _ in range(len(nodes) + 15): 
        lon = np.random.uniform(-150, 150)
        lat = np.random.uniform(-40, 60)
        val = np.random.randint(10, 100)
        map_data.append({"name": "Node", "value": [float(lon), float(lat), 50]})
    df = pd.DataFrame(map_data)
    fig = go.Figure(data=go.Scattergeo(
        lon = [d['value'][0] for d in map_data],
        lat = [d['value'][1] for d in map_data],
        mode = 'markers',
        marker = dict(size=5, color='#ffd60a', opacity=0.8)
    ))
    fig.update_layout(geo = dict(scope='world', projection_type='natural earth', showland=True, landcolor="rgb(20, 20, 20)", bgcolor="black"), margin={"r":0,"t":0,"l":0,"b":0}, paper_bgcolor="black", height=500)
    st.plotly_chart(fig, use_container_width=True)

def render_3d_galaxy(nodes):
    if len(nodes) < 3: st.info("ğŸŒŒ æ˜Ÿæ²³æ±‡èšä¸­..."); return
    vectors, labels, colors = [], [], []
    for i, node in enumerate(nodes):
        if node['vector']:
            try:
                v = json.loads(node['vector'])
                vectors.append(v)
                labels.append(node['care_point'])
                colors.append(i % 3)
            except: pass
    if not vectors: return
    pca = PCA(n_components=3)
    coords = pca.fit_transform(vectors)
    df = pd.DataFrame(coords, columns=['x', 'y', 'z'])
    df['label'] = labels
    df['cluster'] = colors
    fig = px.scatter_3d(df, x='x', y='y', z='z', color='cluster', hover_name='label', template="plotly_dark", opacity=0.8)
    fig.update_layout(scene=dict(xaxis=dict(visible=False), yaxis=dict(visible=False), zaxis=dict(visible=False), bgcolor='black'), paper_bgcolor="black", margin={"r":0,"t":0,"l":0,"b":0}, height=600, showlegend=False)
    st.plotly_chart(fig, use_container_width=True)

def render_radar_chart(radar_dict, height="200px"):
    keys = ["Care", "Curiosity", "Reflection", "Coherence", "Empathy", "Agency", "Aesthetic"]
    scores = [radar_dict.get(k, 3.0) for k in keys]
    option = {
        "backgroundColor": "transparent",
        "radar": {"indicator": [{"name": k, "max": 10} for k in keys], "splitArea": {"show": False}},
        "series": [{"type": "radar", "data": [{"value": scores, "areaStyle": {"color": "rgba(0,255,242,0.4)"}, "lineStyle": {"color": "#00fff2"}}]}]
    }
    st_echarts(options=option, height=height)

def render_cyberpunk_map(nodes, height="250px", is_fullscreen=False):
    if not nodes: return
    graph_nodes, graph_links = [], []
    symbol_base = 30 if is_fullscreen else 15
    for i, node in enumerate(nodes):
        # ğŸŒŸ ä¿®å¤ï¼šç°åœ¨ä¼˜å…ˆå– m_scoreï¼Œæ²¡æœ‰å†å– logic_score
        logic = node.get('m_score') if node.get('m_score') is not None else node.get('logic_score', 0.5)
        keywords = []
        if node.get('keywords'):
            if isinstance(node['keywords'], str): keywords = json.loads(node['keywords'])
            else: keywords = node['keywords']
        vector = None
        if node.get('vector'):
            if isinstance(node['vector'], str): vector = json.loads(node['vector'])
            else: vector = node['vector']

        graph_nodes.append({
            "name": str(node['id']), "id": str(node['id']),
            "symbolSize": symbol_base * (0.8 + logic),
            "value": node['care_point'],
            "label": {"show": is_fullscreen, "formatter": node['care_point'][:5], "color": "#fff"},
            "vector": vector,
            "keywords": keywords
        })
    node_count = len(graph_nodes)
    for i in range(node_count):
        for j in range(i + 1, node_count):
            na, nb = graph_nodes[i], graph_nodes[j]
            if na['vector'] and nb['vector']:
                vec_sim = cosine_similarity(na['vector'], nb['vector'])
                if vec_sim > 0.8: graph_links.append({"source": na['name'], "target": nb['name'], "lineStyle": {"width": 2, "color": "#00fff2"}})
                elif vec_sim > 0.6: graph_links.append({"source": na['name'], "target": nb['name'], "lineStyle": {"width": 0.5, "color": "#555", "type": "dashed"}})
    option = {
        "backgroundColor": "#0e1117",
        "series": [{"type": "graph", "layout": "force", "data": graph_nodes, "links": graph_links, "roam": True, "force": {"repulsion": 1000 if is_fullscreen else 300}, "itemStyle": {"shadowBlur": 10}}]
    }
    st_echarts(options=option, height=height)
