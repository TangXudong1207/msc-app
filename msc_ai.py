import streamlit as st
from openai import OpenAI
import json
import re
import numpy as np
import msc_config as config
import msc_db as db

# ğŸ›‘ AI åˆå§‹åŒ–
try:
    client_ai = OpenAI(api_key=st.secrets["API_KEY"], base_url=st.secrets["BASE_URL"])
    TARGET_MODEL = st.secrets["MODEL_NAME"]
except: st.stop()

# --- å‘é‡ç®—æ³• ---
def get_embedding(text):
    # æ¨¡æ‹Ÿå‘é‡ (å®é™…åº”è°ƒç”¨ API)
    return np.random.rand(1536).tolist()

def cosine_similarity(v1, v2):
    vec1, vec2 = np.array(v1), np.array(v2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)) if np.linalg.norm(vec1) > 0 else 0

# --- æ ¸å¿ƒé€»è¾‘ v70.0 ---

def calculate_novelty_relative(current_vec, username):
    """
    è®¡ç®—ç›¸å¯¹æ–°é¢–åº¦ï¼š1 - ä¸è¿‡å»æœ€è¿‘10ä¸ªèŠ‚ç‚¹çš„å¹³å‡ç›¸ä¼¼åº¦
    """
    recent_nodes = db.get_user_nodes(username) # å‡è®¾è¿”å›æŒ‰æ—¶é—´å€’åº
    if not recent_nodes: return 1.0 # æ²¡å†å²ï¼Œç»å¯¹æ–°é¢–
    
    # å–æœ€è¿‘ 10 æ¡
    check_list = recent_nodes[-10:] if len(recent_nodes) > 10 else recent_nodes
    sims = []
    for node in check_list:
        if node.get('vector'):
            try:
                # ç®€å•æ¨¡æ‹Ÿï¼Œå®é™…åº”ç”¨çœŸå®å‘é‡è®¡ç®—
                # è¿™é‡Œä¸ºäº†æ¼”ç¤ºé€»è¾‘ï¼Œå‡è®¾ get_embedding è¿”å›çš„æ˜¯çœŸå®å¯æ¯”çš„
                # ç”±äºç°åœ¨ get_embedding æ˜¯éšæœºçš„ï¼Œæ‰€ä»¥ sim ä¼šå¾ˆä½ï¼ŒNovelty ä¼šå¾ˆé«˜
                # ç”Ÿäº§ç¯å¢ƒéœ€æ¥çœŸå® Embedding API
                v_old = json.loads(node['vector'])
                sims.append(cosine_similarity(current_vec, v_old))
            except: pass
            
    if not sims: return 1.0
    avg_sim = sum(sims) / len(sims)
    return 1.0 - avg_sim # è¶Šä¸ç›¸ä¼¼ï¼Œæ–°é¢–åº¦è¶Šé«˜

def calculate_m_score(ai_data, n_relative):
    """
    M_score = 0.35*C_emotion + 0.25*C_self + 0.20*N_abstract + 0.20*N_relative
    """
    c_emo = ai_data.get('score_emotion', 0)
    c_self = ai_data.get('score_self', 0)
    n_abs = ai_data.get('score_abstract', 0)
    
    w = config.W_MEANING
    m_score = (w['C_emotion'] * c_emo) + \
              (w['C_self'] * c_self) + \
              (w['N_abstract'] * n_abs) + \
              (w['N_relative'] * n_relative)
    return m_score

# --- LLM è°ƒç”¨ ---
def call_ai_api(prompt):
    try:
        response = client_ai.chat.completions.create(
            model=TARGET_MODEL,
            messages=[{"role": "system", "content": "Output valid JSON only. No markdown."}, {"role": "user", "content": prompt}],
            temperature=0.7, stream=False, response_format={"type": "json_object"} 
        )
        content = response.choices[0].message.content
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group(0))
            else: return json.loads(content)
        except: return {"error": True}
    except Exception as e: return {"error": True, "msg": str(e)}

def analyze_meaning_engine(text, username):
    # 1. æ„é€  Prompt
    prompt = f"""
    {config.PROMPT_ANALYST}
    ç”¨æˆ·è¾“å…¥ï¼š"{text}"
    
    è¿”å› JSON:
    {{
        "score_emotion": 0.0-1.0,
        "score_self": 0.0-1.0,
        "score_abstract": 0.0-1.0,
        "is_existential": true/false,
        "care_point": "...", "meaning_layer": "...", "insight": "...",
        "keywords": ["A", "B", "C"],
        "radar_scores": {{ "Care": 5, "Curiosity": 5, "Reflection": 5, "Coherence": 5, "Empathy": 5, "Agency": 5, "Aesthetic": 5 }}
    }}
    """
    
    # 2. AI åˆ†æ
    ai_res = call_ai_api(prompt)
    if "error" in ai_res: return ai_res

    # 3. è®¡ç®—ç›¸å¯¹æ–°é¢–åº¦ (Pythonç«¯è®¡ç®—)
    current_vec = get_embedding(text)
    n_relative = calculate_novelty_relative(current_vec, username)
    
    # 4. ç»¼åˆè®¡ç®— M_score
    m_score = calculate_m_score(ai_res, n_relative)
    
    # 5. åˆ¤å®šç­‰çº§
    status = "NonMeaning"
    if m_score >= config.LEVELS['Core']: status = "Core"
    elif m_score >= config.LEVELS['Strong']: status = "Strong"
    elif m_score >= config.LEVELS['Weak']: status = "Weak"
    
    # 6. åªæœ‰ Weak ä»¥ä¸Šæ‰ Valid
    ai_res['valid'] = (m_score >= config.LEVELS['NonMeaning'])
    ai_res['m_score'] = m_score
    ai_res['status'] = status
    ai_res['vector'] = current_vec
    
    return ai_res

# ... (å…¶ä»–å‡½æ•°ä¿æŒåŸæ ·: get_normal_response, generate_daily, etc.) ...
def get_normal_response(history_messages):
    try:
        api_messages = [{"role": "system", "content": config.PROMPT_CHATBOT}]
        for msg in history_messages: api_messages.append({"role": msg["role"], "content": msg["content"]})
        return client_ai.chat.completions.create(model=TARGET_MODEL, messages=api_messages, temperature=0.8, stream=True)
    except Exception as e: return str(e)

def generate_daily_question(username, radar_data):
    ctx = ""
    # Simplified context fetching
    prompt = f"{config.PROMPT_DAILY}\nç”¨æˆ·ï¼š{json.dumps(radar_data)}ã€‚{ctx}ã€‚è¾“å‡º JSON: {{ 'question': '...' }}"
    res = call_ai_api(prompt)
    return res.get("question", "ä»Šå¤©æ„Ÿè§‰å¦‚ä½•ï¼Ÿ")

def analyze_persona_report(radar_data):
    prompt = f"{config.PROMPT_PERSONA}\næ•°æ®ï¼š{json.dumps(radar_data)}ã€‚è¾“å‡º JSON: {{ 'static_portrait': '...', 'dynamic_growth': '...' }}"
    return call_ai_api(prompt)

def get_ai_interjection(history_text):
    prompt = f"{config.PROMPT_OBSERVER}\n{history_text}\nè¾“å‡º: çº¯æ–‡æœ¬"
    try: return client_ai.chat.completions.create(model=TARGET_MODEL, messages=[{"role": "user", "content": prompt}], temperature=0.9).choices[0].message.content
    except: return None

def generate_fusion(a, b): return call_ai_api(f"èåˆ {a} å’Œ {b}ã€‚JSON: {{'care_point':'...', 'meaning_layer':'...', 'insight':'...'}}")
def find_resonance(v, u, d): return None # å ä½ï¼Œé€»è¾‘ç§»åˆ° lib
def calculate_rank(d): return "MSC å…¬æ°‘", "ğŸ¥‰" # å ä½
