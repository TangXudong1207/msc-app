import streamlit as st
from openai import OpenAI
import json
import re
import numpy as np
import msc_config as config
import msc_db as db

# ğŸ›‘ AI åˆå§‹åŒ–
try:
    client_ai = OpenAI(api_key=st.secrets["API_KEY"], base_url=st.secrets["BASE_URL"])
    TARGET_MODEL = st.secrets["MODEL_NAME"]
except Exception as e: st.error(f"AI Config Error: {e}"); st.stop()

# --- æ ¸å¿ƒç®—æ³• ---
def get_embedding(text):
    return np.random.rand(1536).tolist()

def cosine_similarity(v1, v2):
    vec1, vec2 = np.array(v1), np.array(v2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)) if np.linalg.norm(vec1) > 0 else 0

def calculate_MLS(vec_a, vec_b, topic_a, topic_b, meaning_a, meaning_b):
    sim_vec = cosine_similarity(vec_a, vec_b)
    # ç®€åŒ–ç‰ˆ MLSï¼Œä¸»è¦ä¾èµ–å‘é‡ç›¸ä¼¼åº¦
    return sim_vec

def find_resonance(current_vector, current_user, current_data):
    if not current_vector: return None
    others = db.get_resonance_candidates(current_user)
    if not others: return None
    
    best_match, highest_score = None, 0
    for row in others:
        if row['vector']:
            try:
                score = cosine_similarity(current_vector, json.loads(row['vector']))
                if score > config.RESONANCE_THRESHOLD and score > highest_score:
                    highest_score = score
                    best_match = {"user": row['username'], "content": row['content'], "score": round(score * 100, 1)}
            except: continue
    return best_match

def calculate_rank(radar_data):
    if not radar_data: return "é’é“œ", "ğŸ¥‰"
    if isinstance(radar_data, str): radar_data = json.loads(radar_data)
    total = sum(radar_data.values())
    if total < 25: return "é’é“œ", "ğŸ¥‰"
    elif total < 38: return "é»„é‡‘", "ğŸ¥‡"
    elif total < 54: return "é’»çŸ³", "ğŸ’ "
    else: return "ç‹è€…", "ğŸ‘‘"

# --- LLM è°ƒç”¨ ---
def call_ai_api(prompt):
    try:
        response = client_ai.chat.completions.create(
            model=TARGET_MODEL,
            messages=[{"role": "system", "content": "Output valid JSON only. No markdown."}, {"role": "user", "content": prompt}],
            temperature=0.7, stream=False, response_format={"type": "json_object"} 
        )
        content = response.choices[0].message.content
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group(0))
            else: return json.loads(content)
        except: return {"error": True}
    except Exception as e: return {"error": True, "msg": str(e)}

def get_normal_response(history_messages):
    try:
        api_messages = [{"role": "system", "content": config.PROMPT_CHATBOT}]
        for msg in history_messages: 
            if msg['role'] in ['user', 'assistant']:
                api_messages.append({"role": msg["role"], "content": msg["content"]})
        return client_ai.chat.completions.create(model=TARGET_MODEL, messages=api_messages, temperature=0.8, stream=True)
    except Exception as e: return str(e)

def analyze_meaning_background(text):
    prompt = f"{config.PROMPT_ANALYST}\nç”¨æˆ·è¾“å…¥: \"{text}\""
    res = call_ai_api(prompt)
    if res.get("valid", False):
        c = res.get('c_score', 0)
        n = res.get('n_score', 0)
        m = c * n * 2
        res['m_score'] = m
        if m < config.MEANING_THRESHOLD: res["valid"] = False
    return res

def generate_daily_question(username, radar_data):
    recent = db.get_all_nodes_for_map(username)
    ctx = ""
    if recent: ctx = f"å…³æ³¨ç‚¹ï¼š{[n['care_point'] for n in recent[-3:]]}"
    prompt = f"{config.PROMPT_DAILY}\nç”¨æˆ·ï¼š{json.dumps(radar_data)}ã€‚{ctx}ã€‚è¾“å‡º JSON: {{ 'question': '...' }}"
    res = call_ai_api(prompt)
    return res.get("question", "ä»Šå¤©æ„Ÿè§‰å¦‚ä½•ï¼Ÿ")

def analyze_persona_report(radar_data):
    prompt = f"{config.PROMPT_PERSONA}\né›·è¾¾æ•°æ®ï¼š{json.dumps(radar_data)}ã€‚è¾“å‡º JSON: {{ 'static_portrait': '...', 'dynamic_growth': '...' }}"
    return call_ai_api(prompt)

def get_ai_interjection(history_text):
    # ä½¿ç”¨ç®€å• Prompt
    prompt = f"ä½œä¸ºè§‚å¯Ÿè€…è¯„è®ºè¿™æ®µå¯¹è¯ï¼š\n{history_text}\nç®€çŸ­å¹½é»˜ã€‚ç›´æ¥è¿”å›æ–‡æœ¬ã€‚"
    try:
        response = client_ai.chat.completions.create(model=TARGET_MODEL, messages=[{"role": "user", "content": prompt}], temperature=0.9)
        return response.choices[0].message.content
    except: return None
